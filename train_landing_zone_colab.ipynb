{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b0af2cd",
   "metadata": {},
   "source": [
    "# Landing Zone Detection with RF-DETR\n",
    "## CS4824 Final Project - Google Colab Training Notebook\n",
    "\n",
    "This notebook trains an RF-DETR model to detect landing zones from Blender-rendered frames.\n",
    "\n",
    "### Instructions:\n",
    "1. Upload `coco-dataset.zip` to your Google Drive (root folder or a specific folder)\n",
    "2. Run this notebook with GPU enabled: Runtime → Change runtime type → GPU (T4)\n",
    "3. Mount Google Drive when prompted\n",
    "4. The notebook will unzip the dataset and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adea41c",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, check GPU availability and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed9ddb7",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive & Load Dataset\n",
    "\n",
    "Mount your Google Drive and unzip the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e5b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set the path to your zip file in Google Drive\n",
    "# Change this path if you put the zip file in a different folder\n",
    "zip_path = '/content/drive/MyDrive/coco-dataset.zip'\n",
    "\n",
    "# Alternative: If you put it in a subfolder, use something like:\n",
    "# zip_path = '/content/drive/MyDrive/CS4824/coco-dataset.zip'\n",
    "\n",
    "import os\n",
    "if os.path.exists(zip_path):\n",
    "    print(f\"✓ Found dataset at: {zip_path}\")\n",
    "else:\n",
    "    print(f\"✗ Dataset not found at: {zip_path}\")\n",
    "    print(\"Please upload coco-dataset.zip to your Google Drive and update the path above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4d248",
   "metadata": {},
   "source": [
    "## 3. Unzip Dataset & Install RF-DETR\n",
    "\n",
    "Extract the dataset and install the RF-DETR library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d177c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the dataset\n",
    "import zipfile\n",
    "\n",
    "zip_path = '/content/drive/MyDrive/coco-dataset.zip'\n",
    "extract_path = '/content'\n",
    "\n",
    "print(\"Extracting dataset...\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "print(\"✓ Dataset extracted!\")\n",
    "\n",
    "# Verify extraction\n",
    "import os\n",
    "dataset_path = '/content/coco-dataset'\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"✓ Dataset directory exists: {dataset_path}\")\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        split_path = os.path.join(dataset_path, split)\n",
    "        if os.path.exists(split_path):\n",
    "            num_files = len(os.listdir(split_path))\n",
    "            print(f\"  {split}: {num_files} files\")\n",
    "else:\n",
    "    print(\"✗ Dataset extraction failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047c90b",
   "metadata": {},
   "source": [
    "## 4. Install RF-DETR\n",
    "\n",
    "Clone and install the RF-DETR library from Roboflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c699fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone and install RF-DETR\n",
    "!git clone https://github.com/roboflow/rf-detr.git\n",
    "%cd rf-detr\n",
    "!pip install -e .\n",
    "%cd /content\n",
    "\n",
    "print(\"\\n✓ RF-DETR installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076cbe87",
   "metadata": {},
   "source": [
    "## 5. Verify Dataset Structure\n",
    "\n",
    "Check that the COCO dataset is properly structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff15ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Check dataset structure\n",
    "dataset_path = \"/content/coco-dataset\"\n",
    "\n",
    "print(\"Dataset structure:\")\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    level = root.replace(dataset_path, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files[:5]:  # Show first 5 files\n",
    "        print(f\"{subindent}{file}\")\n",
    "    if len(files) > 5:\n",
    "        print(f\"{subindent}... and {len(files) - 5} more files\")\n",
    "\n",
    "# Check annotation files\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    anno_file = f\"{dataset_path}/{split}/_annotations.coco.json\"\n",
    "    if os.path.exists(anno_file):\n",
    "        with open(anno_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"\\n{split.upper()} split:\")\n",
    "        print(f\"  Images: {len(data['images'])}\")\n",
    "        print(f\"  Annotations: {len(data['annotations'])}\")\n",
    "        print(f\"  Categories: {data['categories']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77f950e",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample Images\n",
    "\n",
    "Display some training images with their bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a8c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Load training annotations\n",
    "with open('/content/coco-dataset/train/_annotations.coco.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Create image_id to annotations mapping\n",
    "img_to_annos = {}\n",
    "for anno in train_data['annotations']:\n",
    "    img_id = anno['image_id']\n",
    "    if img_id not in img_to_annos:\n",
    "        img_to_annos[img_id] = []\n",
    "    img_to_annos[img_id].append(anno)\n",
    "\n",
    "# Display first 6 images with annotations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_info in enumerate(train_data['images'][:6]):\n",
    "    img_path = f\"/content/coco-dataset/train/{img_info['file_name']}\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    if img_info['id'] in img_to_annos:\n",
    "        for anno in img_to_annos[img_info['id']]:\n",
    "            x, y, w, h = anno['bbox']\n",
    "            rect = Rectangle((x, y), w, h, linewidth=2, edgecolor='lime', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_title(f\"{img_info['file_name']}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b6f4f9",
   "metadata": {},
   "source": [
    "## 7. Train the RF-DETR Model\n",
    "\n",
    "Train the model using the RF-DETR library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea9ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from rfdetr import RFDETRNano\n",
    "from rfdetr.config import TrainConfig\n",
    "\n",
    "def train_landing_zone_model():\n",
    "    \"\"\"Train RF-DETR model for landing zone detection\"\"\"\n",
    "    \n",
    "    dataset_path = \"/content/coco-dataset\"\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"Initializing RF-DETR Nano model...\")\n",
    "    model = RFDETRNano(\n",
    "        class_names=['Landing Zone'],\n",
    "    )\n",
    "    \n",
    "    # Configure training\n",
    "    config = TrainConfig(\n",
    "        data_root=dataset_path,\n",
    "        output_dir=\"/content/output/landing_zone_detection\",\n",
    "        batch_size=8,  # Good batch size for T4 GPU\n",
    "        epochs=100,\n",
    "        learning_rate=0.0001,\n",
    "        num_workers=2,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        image_size=640,\n",
    "        save_interval=10,\n",
    "        eval_interval=5,\n",
    "        warmup_epochs=5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining Configuration:\")\n",
    "    print(f\"  Device: {config.device}\")\n",
    "    print(f\"  Batch size: {config.batch_size}\")\n",
    "    print(f\"  Epochs: {config.epochs}\")\n",
    "    print(f\"  Learning rate: {config.learning_rate}\")\n",
    "    print(f\"  Image size: {config.image_size}\")\n",
    "    print(f\"  Data root: {config.data_root}\")\n",
    "    print(f\"  Output dir: {config.output_dir}\\n\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    model.train(config)\n",
    "    \n",
    "    print(\"\\n✓ Training completed!\")\n",
    "    print(f\"Model saved to: {config.output_dir}\")\n",
    "    \n",
    "    return model, config\n",
    "\n",
    "# Run training\n",
    "model, config = train_landing_zone_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4add3e",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Results\n",
    "\n",
    "Check the output directory for trained model checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf7d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for training outputs\n",
    "import os\n",
    "\n",
    "output_dir = \"/content/output/landing_zone_detection\"\n",
    "\n",
    "# List files in output directory\n",
    "if os.path.exists(output_dir):\n",
    "    print(\"Output directory contents:\")\n",
    "    for item in os.listdir(output_dir):\n",
    "        item_path = os.path.join(output_dir, item)\n",
    "        size = os.path.getsize(item_path) / (1024*1024)  # MB\n",
    "        print(f\"  {item} ({size:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"Output directory not found: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b61c3c",
   "metadata": {},
   "source": [
    "## 9. Test Inference\n",
    "\n",
    "Run inference on test images to see model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a233c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and run inference\n",
    "from rfdetr import RFDETRNano\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import os\n",
    "\n",
    "# Find the latest checkpoint\n",
    "checkpoint_dir = \"/content/output/landing_zone_detection\"\n",
    "checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
    "\n",
    "if checkpoint_files:\n",
    "    latest_checkpoint = sorted(checkpoint_files)[-1]\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = RFDETRNano(\n",
    "        class_names=['Landing Zone'],\n",
    "        pretrain_weights=checkpoint_path\n",
    "    )\n",
    "    \n",
    "    # Test on some images\n",
    "    test_dir = \"/content/coco-dataset/test\"\n",
    "    test_images = [f\"{test_dir}/{f}\" for f in os.listdir(test_dir) if f.endswith('.png')][:6]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, img_path in enumerate(test_images):\n",
    "        img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run inference\n",
    "        predictions = model.predict(img_path, conf_threshold=0.3)\n",
    "        \n",
    "        ax = axes[idx]\n",
    "        ax.imshow(img_rgb)\n",
    "        \n",
    "        # Draw predictions\n",
    "        if predictions and len(predictions) > 0:\n",
    "            for pred in predictions:\n",
    "                if hasattr(pred, 'bbox'):\n",
    "                    x1, y1, x2, y2 = pred.bbox\n",
    "                    w, h = x2 - x1, y2 - y1\n",
    "                    rect = Rectangle((x1, y1), w, h, linewidth=2, edgecolor='red', facecolor='none')\n",
    "                    ax.add_patch(rect)\n",
    "                    if hasattr(pred, 'confidence'):\n",
    "                        ax.text(x1, y1-5, f\"{pred.confidence:.2f}\", color='red', fontsize=10, weight='bold')\n",
    "        \n",
    "        ax.set_title(os.path.basename(img_path))\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No checkpoint files found. Please train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beba7b6",
   "metadata": {},
   "source": [
    "## 10. Save Model to Google Drive\n",
    "\n",
    "Save the trained model back to Google Drive for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e707b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model to Google Drive\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "output_dir = \"/content/output/landing_zone_detection\"\n",
    "drive_output = \"/content/drive/MyDrive/landing_zone_model\"\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    # Create directory in Drive\n",
    "    os.makedirs(drive_output, exist_ok=True)\n",
    "    \n",
    "    # Copy all files\n",
    "    for item in os.listdir(output_dir):\n",
    "        src = os.path.join(output_dir, item)\n",
    "        dst = os.path.join(drive_output, item)\n",
    "        if os.path.isfile(src):\n",
    "            shutil.copy2(src, dst)\n",
    "            print(f\"✓ Copied: {item}\")\n",
    "    \n",
    "    print(f\"\\n✓ Model saved to Google Drive: {drive_output}\")\n",
    "    \n",
    "    # Also create a zip for easy download\n",
    "    shutil.make_archive(\"/content/drive/MyDrive/landing_zone_model_archive\", 'zip', output_dir)\n",
    "    print(\"✓ Created zip archive: /content/drive/MyDrive/landing_zone_model_archive.zip\")\n",
    "else:\n",
    "    print(\"Output directory not found. Train the model first.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
