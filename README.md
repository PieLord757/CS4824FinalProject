<div id="top">

<!-- HEADER STYLE: CLASSIC -->
<div align="center">

<img src="drone_landing_logo.png" width="30%" style="position: relative; top: 0; right: 0;" alt="Project Logo"/>

# DRONE LANDING ZONE DETECTION â€“ CS 4824 Final Project

<em>See Safe Ground Before You Touch Down</em>

<!-- BADGES -->
<img src="https://img.shields.io/github/license/PieLord757/CS4824FinalProject?style=flat&logo=opensourceinitiative&logoColor=white&color=0080ff" alt="license">
<img src="https://img.shields.io/github/last-commit/PieLord757/CS4824FinalProject?style=flat&logo=git&logoColor=white&color=0080ff" alt="last-commit">
<img src="https://img.shields.io/github/languages/top/PieLord757/CS4824FinalProject?style=flat&color=0080ff" alt="repo-top-language">
<img src="https://img.shields.io/github/languages/count/PieLord757/CS4824FinalProject?style=flat&color=0080ff" alt="repo-language-count">

<em>Built with the tools and technologies:</em><br>

<img src="https://img.shields.io/badge/Python-3776AB.svg?style=flat&logo=Python&logoColor=white" alt="Python">
<img src="https://img.shields.io/badge/Jupyter-F37626.svg?style=flat&logo=Jupyter&logoColor=white" alt="Jupyter">
<img src="https://img.shields.io/badge/Roboflow-111827.svg?style=flat&logo=roboflow&logoColor=white" alt="Roboflow">
<img src="https://img.shields.io/badge/COCO%20Format-000000.svg?style=flat&logo=json&logoColor=white" alt="COCO">
<img src="https://img.shields.io/badge/PyTorch-EE4C2C.svg?style=flat&logo=PyTorch&logoColor=white" alt="PyTorch"><br>
<img src="https://img.shields.io/badge/Blender-F5792A.svg?style=flat&logo=Blender&logoColor=white" alt="Blender">
<img src="https://img.shields.io/badge/Google%20Colab-F9AB00.svg?style=flat&logo=googlecolab&logoColor=white" alt="Google Colab">
<img src="https://img.shields.io/badge/OpenCV-27338e.svg?style=flat&logo=OpenCV&logoColor=white" alt="OpenCV">

</div>
<br>

---

## ğŸ“„ Table of Contents

- [Overview](#-overview)
- [Problem \& Motivation](#-problem--motivation)
- [What This Project Does](#-what-this-project-does)
- [Dataset \& Labeling](#-dataset--labeling)
- [Model \& Approach](#-model--approach)
- [Project Structure](#-project-structure)
- [Getting Started](#-getting-started)
  - [Prerequisites](#-prerequisites)
  - [Installation](#-installation)
  - [Data Preparation](#-data-preparation)
  - [Training](#-training)
  - [Inference](#-inference)
- [Results \& Evaluation](#-results--evaluation)
- [Challenges](#-challenges)
- [What We Learned](#-what-we-learned)
- [Limitations \& Future Work](#-limitations--future-work)
- [Tech Stack](#-tech-stack)
- [Contributors](#-contributors)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)

---

## âœ¨ Overview

This repository contains our **CS 4824 â€“ Machine Learning** final project: a **computer vision system** that detects **safe landing zones for drones / VTOL craft** from images.

The goal is to build an end-to-end pipeline that:

- Takes **aerial or near-ground images** from drone-like viewpoints  
- Uses a modern **object detection model** to localize **landing zones**  
- Trains and evaluates the model using a **COCO-formatted dataset**  
- Can be extended to work with **synthetic scenes (Blender) and real-world photos**

We use **Roboflowâ€™s RF-DETR** object detector and a **COCO dataset** to explore how well a model can learn to recognize â€œsafe places to landâ€ under diverse environments.

---

## ğŸ’¡ Problem & Motivation

Autonomous drones and VTOL vehicles must often land in **unstructured, cluttered environments**:

- Rooftops, parking lots, courtyards, stadiums  
- Urban areas with people, cars, obstacles, wires  
- Remote areas with uneven ground or vegetation  

In many practical scenarios (delivery, inspection, search and rescue), the drone has to **choose a safe landing zone on its own**. Misclassification is dangerous:

- A **false positive** (unsafe area labeled safe) can lead to damage or injury  
- A **false negative** limits where the drone can land and reduces mission flexibility  

We ask:

> **Can we train a robust detector that reliably finds safe landing zones across diverse scenes and viewpoints?**

---

## ğŸ—ï¸ What This Project Does

At a high level, this repository provides:

- ğŸ§¹ **Data Processing**  
  - Scripts to preprocess raw landing zone images  
  - Conversion of custom annotations into **COCO** format

- ğŸ·ï¸ **Dataset Conversion**  
  - A script to **convert landing zone annotations â†’ COCO JSON** compatible with RF-DETR and other detectors

- ğŸ§  **Model Training**  
  - A training script that uses **`config_landing_zone.yaml`** to train a model on the landing zone dataset  
  - A **Colab notebook** to run training in the cloud (GPU)

- âœ… **Verification & Setup**  
  - A helper script to verify that your environment, dependencies, and dataset paths are correctly configured

Overall, the repository is focused on **reproducible training** and **clean data handling** for the drone landing zone detection task.

---

## ğŸ—‚ï¸ Dataset & Labeling

The dataset is stored as a compressed archive:

- **`coco-dataset.zip`** â€“ COCO-style dataset containing:
  - Images of potential landing zones from diverse environments  
  - Bounding box annotations marking **landing zone regions**

### Sources & scenes

The project is designed to support images from:

- **Synthetic scenes** created in **Blender** (e.g., rooftops, fields, rooftops with helipads)  
- **Prompt-generated images** (e.g., Midjourney / similar tools)  
- **Real-world-like drone views** (city plazas, rooftops, open fields, campuses)

### Annotation format

The dataset uses **COCO** object detection format, with:

- `images` â€“ metadata and file paths  
- `annotations` â€“ bounding boxes + category id for **landing zones**  
- `categories` â€“ class definitions (`landing_zone`, and optionally others)

You can regenerate or modify the COCO annotations using:

- **`convert_to_coco.py`** â€“ converts raw annotation formats into a unified COCO JSON  
- **`process_landing_zones.py`** â€“ optional additional cleaning / preparation logic

---

## ğŸ§  Model & Approach

This project is designed around **RF-DETR**, Roboflowâ€™s implementation of the **DETR-style transformer-based object detector**:

- End-to-end detection (no anchors, no NMS)  
- Strong performance on crowded scenes  
- Works well with **COCO** and custom datasets  

The core configuration lives in:

- **`config_landing_zone.yaml`**

That config typically includes (you can open it and adjust):

- Paths to training / validation COCO files  
- Image sizes and augmentations  
- Model backbone / architecture choices  
- Training hyperparameters (epochs, batch size, learning rate, etc.)

Training logic is handled by:

- **`train_landing_zone.py`** â€“ Python script for local training  
- **`train_landing_zone_colab.ipynb`** â€“ Jupyter notebook for **Google Colab** training

---

## ğŸ“ Project Structure

```text
CS4824FinalProject/
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore
â”œâ”€â”€ coco-dataset.zip              # COCO dataset archive (images + JSON)
â”œâ”€â”€ config_landing_zone.yaml      # RF-DETR / training configuration
â”œâ”€â”€ convert_to_coco.py            # Convert raw annotations into COCO format
â”œâ”€â”€ process_landing_zones.py      # Preprocess / clean landing zone datasets
â”œâ”€â”€ train_landing_zone.py         # Main training script (Python)
â”œâ”€â”€ train_landing_zone_colab.ipynb# Colab notebook for training in the cloud
â””â”€â”€ verify_and_setup.py           # Environment + dataset verification helper
```

## Quick file descriptions
	â€¢	config_landing_zone.yaml
Central config for the landing zone detector: dataset paths, model config, hyperparameters.
	â€¢	convert_to_coco.py
Script to take your raw labels (e.g., from a specific tool / format) and convert them into a standard COCO JSON.
	â€¢	process_landing_zones.py
Helper script for cleaning, normalizing, or restructuring landing zone data before conversion / training.
	â€¢	train_landing_zone.py
Training entrypoint. Uses the config file and your COCO dataset to train an object detection model.
	â€¢	train_landing_zone_colab.ipynb
Notebook version of training that you can run on Google Colab, with cells for mounting Drive, setting config, and kicking off training.
	â€¢	verify_and_setup.py
Utility script to sanity-check the environment, dataset paths, and any prerequisites before training.

## ğŸ“Š Results & Evaluation

We evaluate the landing zone detector using standard object detection metrics:
	â€¢	mAP@0.5
	â€¢	mAP@0.5:0.95
	â€¢	Precision / Recall curves
	â€¢	Qualitative inspection of predictions on held-out images

Key analysis directions you can explore:
	â€¢	Performance across:
	â€¢	Different scene types (urban vs field vs rooftop)
	â€¢	Different viewing angles (high drone view vs closer ground-level view)
	â€¢	Synthetic vs more realistic images

Add your own:
	â€¢	Plots of mAP vs epoch
	â€¢	Side-by-side examples of ground truth vs predictions
	â€¢	Error analysis (where the model fails, failure modes: clutter, shadows, small landing zones, etc.)

â¸»

## ğŸ§— Challenges

Some challenges we faced while building this project:
	â€¢	Defining â€œlanding zoneâ€ consistently
	â€¢	Itâ€™s not a simple object like â€œcarâ€; itâ€™s a region that must be flat, open, and safe.
	â€¢	Viewpoint variation
	â€¢	Drones can be high overhead, low and close, or at oblique angles.
	â€¢	Synthetic vs real
	â€¢	Synthetic training data from tools like Blender or AI image generators is powerful, but may not perfectly match real-world textures and clutter.
	â€¢	Balancing dataset variety vs label noise
	â€¢	Pushing for diversity in scenes often introduces noisier labels or ambiguous landing areas.

â¸»

## ğŸ“š What We Learned
	â€¢	How to design a dataset and annotation scheme for a region-based concept like â€œlanding zoneâ€
	â€¢	How to convert custom labels into robust COCO format using Python scripts
	â€¢	How to configure and run RF-DETR / modern detection models on a custom dataset
	â€¢	Practical experience with:
	â€¢	Training on Colab
	â€¢	Managing configs
	â€¢	Debugging dataset path / format issues

â¸»

## ğŸ”® Limitations & Future Work

Future directions and potential improvements:
	â€¢	ğŸ” Segmentation instead of bounding boxes
	â€¢	Use semantic / instance segmentation for more precise landing area boundaries.
	â€¢	ğŸ¥ Video + temporal consistency
	â€¢	Smooth predictions across frames for more stable drone decision-making.
	â€¢	ğŸŒ¦ï¸ More realistic data
	â€¢	Mix in real drone captures, varied weather, nighttime scenes, and motion blur.
	â€¢	ğŸš Integration with control systems
	â€¢	Use the detectorâ€™s output to feed a simulated or real drone control policy.
	â€¢	ğŸ§ª Robustness tests
	â€¢	Evaluate on held-out city / country / environment types.

â¸»

## ğŸ§© Tech Stack

Core
	â€¢	Python
	â€¢	Jupyter / Google Colab

Computer Vision
	â€¢	RF-DETR (Roboflow DETR-based object detector)
	â€¢	COCO dataset format
	â€¢	OpenCV (preprocessing & visualization)

Data & Config
	â€¢	YAML for configuration (config_landing_zone.yaml)
	â€¢	Zip archive dataset (coco-dataset.zip)

Tools Used in the Workflow (outside the repo)
	â€¢	Roboflow for dataset management / labeling
	â€¢	Blender for synthetic scene generation
	â€¢	Prompt-based image generation tools for synthetic drone views

â¸»

## ğŸ‘¥ Contributors

Update with your own names / roles.

	â€¢	Student Team â€“ CS 4824
 

Feel free to add GitHub profile links here.

â¸»

## ğŸ“œ License

This project uses the license specified in the repository (see LICENSEï¿¼ if present).

If youâ€™re adapting this project, we recommend adding an explicit license (e.g. MIT) to clarify reuse.

â¸»

## âœ¨ Acknowledgments
	â€¢	CS 4824 (Machine Learning) course staff for project guidance
	â€¢	Roboflow & RF-DETR for tooling and open-source resources
	â€¢	Blender & the open-source community for 3D tools used to generate synthetic environments
	â€¢	Classmates and friends who provided feedback on the landing zone definitions and demo scenes